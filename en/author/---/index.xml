<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>--- | PRIS-NLP Group</title>
    <link>https://pris-nlp.github.io/en/author/---/</link>
      <atom:link href="https://pris-nlp.github.io/en/author/---/index.xml" rel="self" type="application/rss+xml" />
    <description>---</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© PRIS-NLP 2023</copyright><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pris-nlp.github.io/images/icon_hu316c76060153165bc5df614349754290_164774_512x512_fill_lanczos_center_3.png</url>
      <title>---</title>
      <link>https://pris-nlp.github.io/en/author/---/</link>
    </image>
    
    <item>
      <title>Recent research overview（2022.10.01）</title>
      <link>https://pris-nlp.github.io/en/post/221001wyj/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/221001wyj/</guid>
      <description>&lt;h2 id=&#34;distribution-calibration-for-out-of-domain-detection-with-bayesian-approximation&#34;&gt;&lt;strong&gt;Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%90%B4%E4%BA%9A%E6%A5%A0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9B%BE%E8%87%B4%E8%BF%9C/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref2:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%89%9F%E5%AE%87%E6%BB%94/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E9%9C%88/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王霈&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Long Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;域外(out-of-domain, OOD)检测是任务型对话系统应对开放世界的重要要求，在正确识别预定义意图基础上，旨在检测用户输入query是否超出预定义意图。然而，传统的基于softmax的域外检测算法在域外样本识别上存在过自信问题。本文结合理论分析发现，训练分布与测试分布之间不匹配，使得模型不能自信地做出预测，从而导致softmax分数异常，换句话说，缓解分布不确定性是解决过自信问题的关键。基于此，我们提出了基于贝叶斯估计的域外检测算法，在推理阶段，利用蒙特卡罗Dropout多次传导求期望，以校准分布不确定性，将实际错误的尖锐域外分布校准为接近理想的均匀分布，以便更好的区分域内和域外数据。我们的方法灵活适用于现有的基于概率的域外检测算法，相比于MSP，我们的方法在不增加任何模型参数，仅增加0.41%推理时间消耗上，可以获得了33.33%的OOD F1指标提升。文章中大量的实验和分析进一步证明了贝叶斯学习在OOD检测中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;模型整体结构&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码&#34;&gt;代码&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/pris-nlp/COLING2022_Bayesian-for-OOD/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;generalized-intent-discovery-learning-from-open-world-dialogue-system&#34;&gt;&lt;strong&gt;Generalized Intent Discovery: Learning from Open World Dialogue System&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者-1&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%89%9F%E5%AE%87%E6%BB%94/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;&lt;sup id=&#34;fnref3:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref4:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%90%B4%E4%BA%9A%E6%A5%A0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E9%9C%88/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王霈&lt;/a&gt;，王金刚，武威，黄毅，冯俊兰，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref1:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议-1&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Long Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介-1&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;传统的意图分类模型基于领域专家预定义好的域内（IND）意图集，并且只能识别有限的域内意图。然而当对话系统线上部署之后，用户可能会输入域外（OOD）查询。这些OOD数据为系统未来进一步开发提供潜在发展方向。在本文中，我们定义了一个新任务——泛化意图发现（GID），旨在将IND意图分类器的识别范围扩展到包括IND意图和新出现的OOD意图的开放世界意图集中。具体地，我们希望训练一个统一的模型以端到端的方式同时学习分类有标注的域内意图类，以及增量式地从无标注OOD数据中发现和识别的新的域外意图类，以便自动扩展分类器的识别范围，通俗地说就是自动扩充分类头。为了能够在更真实地场景下讨论GID任务，我们构建了三个基准数据集，分别模拟单领域，多领域和跨领域场景，并在此基础上衍生出两个变体数据集，分别模拟真实环境中OOD噪声和OOD类别不平衡场景。在方法上，我们提出了两个框架，pipeline和端到端框架，以便后续GID研究使用。此外，我们还做了大量地分析实验，综合地讨论和分析了GID任务的关键挑战，并为这个方向未来的研究提供了新的指导。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2-1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;GID任务示意图&lt;/font&gt;&lt;/center&gt;
&lt;img src=&#34;2-2.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;pipeline和end-to-end框架示意图&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码-1&#34;&gt;代码&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/myt517/GID_benchmark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pssat-a-perturbed-semantic-structure-awareness-transferring-method-for-perturbation-robust-slot-filling&#34;&gt;&lt;strong&gt;PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者-2&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E8%91%A3%E5%86%A0%E9%9C%86/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;董冠霆&lt;/a&gt;&lt;sup id=&#34;fnref5:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E9%83%AD%E5%B2%B1%E9%A9%B0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郭岱驰&lt;/a&gt;&lt;sup id=&#34;fnref6:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E7%A4%BC%E6%96%87/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王礼文&lt;/a&gt;&lt;sup id=&#34;fnref7:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9D%8E%E9%9B%AA%E5%B3%B0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;李雪峰&lt;/a&gt;&lt;sup id=&#34;fnref8:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E6%B3%BD%E6%99%A8/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王泽晨&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9B%BE%E6%99%A8/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾晨&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，赵金政，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E9%9B%B7%E6%B5%A9/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;雷浩&lt;/a&gt;，崔馨月，黄毅，冯俊兰, 
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref2:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议-2&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Short Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介-2&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;大多数现有的槽填充模型倾向于从训练数据中记忆实体的固有模式与对应的上下文。然而，当这些模型暴露于真实场景下的口语扰动时，可能会导致性能下降或产生期望之外的输出。我们提出了一种扰动语义结构感知迁移方法，来训练出对扰动具有鲁棒性的槽填充模型。具体来说，我们引入了两种基于MLM的训练策略，分别从含有无监督语言扰动的语料库中学习上下文语义结构和实体分布。然后，将上游预训练过程中学习到的语义知识迁移到原始样本中，并通过一致性处理模块过滤低质量的增强数据。这些程序旨在提高槽填充模型的鲁棒性。实验结果表明，我们的方法始终优于以往的基本方法，并在防止模型记忆实体和上下文固有模式的同时，获得了较强的泛化性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3-1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;PSSAT框架示意图&lt;/font&gt;&lt;/center&gt;
&lt;img src=&#34;3-2.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;扰动鲁棒填槽框架示意图&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;文章链接&#34;&gt;文章链接&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/2208.11508&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Equal Contribution&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref4:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref5:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref6:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref7:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref8:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Corresponding Author&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recent research overview（2022.04.08）</title>
      <link>https://pris-nlp.github.io/en/post/220408wzc/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/220408wzc/</guid>
      <description>&lt;h2 id=&#34;adpl-adversarial-prompt-based-domain-adaptation-for-dialogue-summarization-with-knowledge-disentanglement&#34;&gt;&lt;strong&gt;ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weihao-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾伟豪&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/ruotong-geng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;耿若彤&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://sigir.org/sigir2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SIGIR 2022&lt;/a&gt; Full Paper&lt;/p&gt;
&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;领域自适应是机器学习中的一个基本任务。在本文中，我们研究对话摘要任务中的领域迁移问题，试图借助源域的有标注数据迁移到无标注或少标注的目标域，进而提升低资源目标域下对话摘要的生成效果，可用于解决实际场景中小业务数据匮乏的挑战。传统的对话摘要领域迁移方法往往依赖于大规模领域语料，借助于预训练来学习领域间知识。该方法的缺点是实际语料收集难，对算力要求高，针对每一个目标域都需要进行耗时的预训练过程，效率低。因此，本文从微调的角度出发，提出了一种轻量级的解耦知识迁移方法ADPL，无需大规模的预训练过程，仅仅利用源域数据和少量的无标注目标域数据，即可实现高质量的对话摘要生成。具体来说，我们基于prompt learning的思想，针对对话摘要任务中的领域迁移问题，提出了三种特定的prompt结构：domain-invariant prompt (DIP), domain-specific prompt (DSP), 和task-oriented prompt (TOP)，其中 DIP 用来捕获领域间的共享特征，DSP用来建模领域特有知识，TOP用来促进生成流畅的摘要。在训练中，我们仅仅更新这些prompt相关的参数就可以实现领域间知识的解耦和迁移，相比较之前的预训练方法，训练高效环保，对机器的显存要求显著降低。同时，我们基于两个大规模的对话摘要数据集QMSum和TODSum构建了对话摘要领域迁移评测集，在两个评测集上取得了一致的最优效果，实验结果和消融分析都证明了本文提出方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;ADPL整体框架示意图&lt;/font&gt;&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;revisit-overconfidence-for-ood-detection-reassigned-contrastive-learning-with-adaptive-class-dependent-threshold&#34;&gt;&lt;strong&gt;Revisit Overconfidence for OOD Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-1&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref2:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref3:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yuanmeng-yan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;严渊蒙&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/qixiang-gao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;高琪翔&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/zhiyuan-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref1:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-1&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://2022.naacl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAACL 2022&lt;/a&gt; Main Conference, Long Paper&lt;/p&gt;
&lt;h4 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;在面向任务的对话系统中，域外意图（out-of-domain, OOD）检测是必不可少的。它旨在检测用户查询是否超出预定义意图范围（in-domain, IND），以避免执行错误的操作。由于OOD意图标注的复杂性，大多数工作都集中在无监督OOD检测上，即没有标记OOD数据，只有标记IND数据。而当前无监督域外检测方法均忽略了域外检测中的关键性挑战——神经网络的过自信问题。&lt;/p&gt;
&lt;p&gt;在本文中，我们对过自信问题进行了深入分析，并将其拆解为两方面：过自信OOD和过自信IND。基于此，我们分别提出了一种新的重分配对比学习(RCL)来区分语义相似的IND类别之间的意图表示，以缓解过自信OOD问题，以及一种自适应的类局部阈值机制来区分相似的IND和OOD样本，以缓解过自信IND问题。&lt;/p&gt;
&lt;p&gt;具体来说，对于过自信OOD问题，我们首先使用预训练的意图分类器在易混淆的IND类型中构建hard对比对（其中，具有相同真实标签但不同的预测标签称为hard positive pair，不同真实标签但相同预测标签称为hard negative pair），然后，基于构建的对比对训练一个新的模型，并通过监督对比学习来学习相似IND类别的鉴别性意图表示；对于过自信IND问题，不同于传统MSP、GDA使用单一全局阈值的方式，为考虑IND和OOD类别之间的关联性，我们为每个意图类别自适应赋予一个独立阈值，有效区分语义高度相似的IND和OOD样本，缓解过自信OOD问题。多个数据集上实验和分析证明了本文提出方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;domain-oriented-prefix-tuning-towards-efficient-and-generalizable-fine-tuning-for-zero-shot-dialogue-summarization&#34;&gt;&lt;strong&gt;Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-2&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;&lt;sup id=&#34;fnref4:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;&lt;sup id=&#34;fnref5:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weihao-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾伟豪&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref2:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-2&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://2022.naacl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAACL 2022&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;introduction-2&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;现实生活中经常面临到新领域中数据标注稀缺的问题，为新领域进行标注耗时耗力，因此利用有限的源域注释数据为目标域开发低资源对话摘要模型至关重要。当前的生成式对话摘要方法缺乏对新领域的泛化能力，而现有的在摘要领域自适应问题上的研究通常是依赖于大规模的二次预训练。&lt;/p&gt;
&lt;p&gt;为了探索对话摘要领域自适应的轻量级微调方法，在本文中，我们提出了一种高效且可泛化的面向领域的Prefix-tuning模型（Domain-Oriented Prefix-tuning，DOP），在冻结的预训练模型的基础上，结合连续的prefix和离散的prompt表示，提高模型的领域自适应能力。具体来说，我们使用无监督 LDA提取的领域词来初始化连续提示向量，以获得前缀模块的初始参数和表示。我们还添加了一个面向领域的键值对前缀序列来增强经典注意力层，以交互方式获取知识并实现优化。除此之外，我们使用dialogue state和query作为离散prompt，引导模型关注对话中关键内容并增强对新领域的泛化能力。&lt;/p&gt;
&lt;p&gt;我们在两个多域对话摘要数据集 TODSum 和 QMSum 上进行零样本迁移实验并建立领域自适应benchmark， 充分的实验和定性分析证明了我们方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;disentangled-knowledge-transfer-for-ood-intent-discovery-with-unifified-contrastive-learning&#34;&gt;&lt;strong&gt;Disentangled Knowledge Transfer for OOD Intent Discovery with Unifified Contrastive Learning&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-3&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yutao-mu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;&lt;sup id=&#34;fnref6:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref7:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref8:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/zhiyuan-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/hong-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐红&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref3:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-3&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.2022.aclweb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACL 2022&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;introduction-3&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;本文研究域外(OOD)意图发现任务，该任务旨在将新出现的未知意图的样本按意图语义聚成不同类簇，这有助于任务型对话系统发展新的技能。不同于传统的文本聚类任务，域外意图发现需要考虑如何利用已知的域内(IND)意图类别的先验知识，帮助新意图的聚类。因此相关方法都遵循一个两阶段框架：IND预训练和OOD聚类。其中的关键挑战在于如何将IND先验知识迁移到OOD聚类上。之前的方法普遍存在一个问题，将IND预训练过程当作分类任务，采用一个交叉熵分类损失，模型学习到如何分类IND样本，但是下游我们需要对OOD聚类。两个阶段不同的学习目标使得存在一个天然的语义鸿沟，使得IND到OOD的知识迁移变得困难。此外，我们观察到之前的方法只迁移BERT输出的一个共享意图表征，考虑到表征具有高度耦合性，这样一个表征可能不利于OOD聚类。例如，在IND预训练的编码器中存在实例级(instance-level)和聚类级(class-level)的知识，解耦不同级别的知识有助于更好地知识迁移。为了解决这样的问题，我们建立了一个统一的多头对比学习框架，并在此基础上提出了一个新颖的解耦知识迁移方法(DKT)，以便迁移解耦IND意图知识用于OOD聚类。我们意在弥合IND预训练和OOD聚类两个阶段的语义鸿沟。两个基准数据集的实验和分析显示了我们方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Equal Contribution&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref4:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref5:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref6:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref7:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref8:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Corresponding Author&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
