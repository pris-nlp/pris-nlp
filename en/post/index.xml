<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>All Blogs | PRIS-NLP Group</title>
    <link>https://pris-nlp.github.io/en/post/</link>
      <atom:link href="https://pris-nlp.github.io/en/post/index.xml" rel="self" type="application/rss+xml" />
    <description>All Blogs</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© PRIS-NLP 2024</copyright><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pris-nlp.github.io/images/icon_hufe1d734684b71dfae0212a0699968c34_684004_512x512_fill_lanczos_center_3.png</url>
      <title>All Blogs</title>
      <link>https://pris-nlp.github.io/en/post/</link>
    </image>
    
    <item>
      <title>Recent research overview（2022.10.01）</title>
      <link>https://pris-nlp.github.io/en/post/221001wyj/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/221001wyj/</guid>
      <description>&lt;h2 id=&#34;distribution-calibration-for-out-of-domain-detection-with-bayesian-approximation&#34;&gt;&lt;strong&gt;Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%90%B4%E4%BA%9A%E6%A5%A0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9B%BE%E8%87%B4%E8%BF%9C/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref2:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%89%9F%E5%AE%87%E6%BB%94/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E9%9C%88/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王霈&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Long Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;域外(out-of-domain, OOD)检测是任务型对话系统应对开放世界的重要要求，在正确识别预定义意图基础上，旨在检测用户输入query是否超出预定义意图。然而，传统的基于softmax的域外检测算法在域外样本识别上存在过自信问题。本文结合理论分析发现，训练分布与测试分布之间不匹配，使得模型不能自信地做出预测，从而导致softmax分数异常，换句话说，缓解分布不确定性是解决过自信问题的关键。基于此，我们提出了基于贝叶斯估计的域外检测算法，在推理阶段，利用蒙特卡罗Dropout多次传导求期望，以校准分布不确定性，将实际错误的尖锐域外分布校准为接近理想的均匀分布，以便更好的区分域内和域外数据。我们的方法灵活适用于现有的基于概率的域外检测算法，相比于MSP，我们的方法在不增加任何模型参数，仅增加0.41%推理时间消耗上，可以获得了33.33%的OOD F1指标提升。文章中大量的实验和分析进一步证明了贝叶斯学习在OOD检测中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;模型整体结构&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码&#34;&gt;代码&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/pris-nlp/COLING2022_Bayesian-for-OOD/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;generalized-intent-discovery-learning-from-open-world-dialogue-system&#34;&gt;&lt;strong&gt;Generalized Intent Discovery: Learning from Open World Dialogue System&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者-1&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%89%9F%E5%AE%87%E6%BB%94/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;&lt;sup id=&#34;fnref3:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref4:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%90%B4%E4%BA%9A%E6%A5%A0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E9%9C%88/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王霈&lt;/a&gt;，王金刚，武威，黄毅，冯俊兰，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref1:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议-1&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Long Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介-1&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;传统的意图分类模型基于领域专家预定义好的域内（IND）意图集，并且只能识别有限的域内意图。然而当对话系统线上部署之后，用户可能会输入域外（OOD）查询。这些OOD数据为系统未来进一步开发提供潜在发展方向。在本文中，我们定义了一个新任务——泛化意图发现（GID），旨在将IND意图分类器的识别范围扩展到包括IND意图和新出现的OOD意图的开放世界意图集中。具体地，我们希望训练一个统一的模型以端到端的方式同时学习分类有标注的域内意图类，以及增量式地从无标注OOD数据中发现和识别的新的域外意图类，以便自动扩展分类器的识别范围，通俗地说就是自动扩充分类头。为了能够在更真实地场景下讨论GID任务，我们构建了三个基准数据集，分别模拟单领域，多领域和跨领域场景，并在此基础上衍生出两个变体数据集，分别模拟真实环境中OOD噪声和OOD类别不平衡场景。在方法上，我们提出了两个框架，pipeline和端到端框架，以便后续GID研究使用。此外，我们还做了大量地分析实验，综合地讨论和分析了GID任务的关键挑战，并为这个方向未来的研究提供了新的指导。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2-1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;GID任务示意图&lt;/font&gt;&lt;/center&gt;
&lt;img src=&#34;2-2.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;pipeline和end-to-end框架示意图&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码-1&#34;&gt;代码&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/myt517/GID_benchmark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pssat-a-perturbed-semantic-structure-awareness-transferring-method-for-perturbation-robust-slot-filling&#34;&gt;&lt;strong&gt;PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;论文作者-2&#34;&gt;论文作者&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/author/%E8%91%A3%E5%86%A0%E9%9C%86/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;董冠霆&lt;/a&gt;&lt;sup id=&#34;fnref5:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E9%83%AD%E5%B2%B1%E9%A9%B0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郭岱驰&lt;/a&gt;&lt;sup id=&#34;fnref6:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E7%A4%BC%E6%96%87/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王礼文&lt;/a&gt;&lt;sup id=&#34;fnref7:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9D%8E%E9%9B%AA%E5%B3%B0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;李雪峰&lt;/a&gt;&lt;sup id=&#34;fnref8:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E7%8E%8B%E6%B3%BD%E6%99%A8/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;王泽晨&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E6%9B%BE%E6%99%A8/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾晨&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E4%BD%95%E5%8F%AF%E6%B8%85/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，赵金政，
&lt;a href=&#34;https://pris-nlp.github.io/author/%E9%9B%B7%E6%B5%A9/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;雷浩&lt;/a&gt;，崔馨月，黄毅，冯俊兰, 
&lt;a href=&#34;https://pris-nlp.github.io/author/%E5%BE%90%E8%94%9A%E7%84%B6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt; &lt;sup id=&#34;fnref2:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;发表会议-2&#34;&gt;发表会议&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://coling2022.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COLING2022&lt;/a&gt; Short Paper&lt;/p&gt;
&lt;h4 id=&#34;论文简介-2&#34;&gt;论文简介&lt;/h4&gt;
&lt;p&gt;大多数现有的槽填充模型倾向于从训练数据中记忆实体的固有模式与对应的上下文。然而，当这些模型暴露于真实场景下的口语扰动时，可能会导致性能下降或产生期望之外的输出。我们提出了一种扰动语义结构感知迁移方法，来训练出对扰动具有鲁棒性的槽填充模型。具体来说，我们引入了两种基于MLM的训练策略，分别从含有无监督语言扰动的语料库中学习上下文语义结构和实体分布。然后，将上游预训练过程中学习到的语义知识迁移到原始样本中，并通过一致性处理模块过滤低质量的增强数据。这些程序旨在提高槽填充模型的鲁棒性。实验结果表明，我们的方法始终优于以往的基本方法，并在防止模型记忆实体和上下文固有模式的同时，获得了较强的泛化性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3-1.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;PSSAT框架示意图&lt;/font&gt;&lt;/center&gt;
&lt;img src=&#34;3-2.png&#34; alt=&#34;&#34;&gt;&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;扰动鲁棒填槽框架示意图&lt;/font&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h4 id=&#34;文章链接&#34;&gt;文章链接&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/2208.11508&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Equal Contribution&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref4:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref5:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref6:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref7:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref8:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Corresponding Author&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Debiased Contrastive Learning of Unsupervised Sentence Representations</title>
      <link>https://pris-nlp.github.io/en/post/220519wzc/</link>
      <pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/220519wzc/</guid>
      <description>&lt;h1 id=&#34;debiased-contrastive-learning-of-unsupervised-sentence-representations&#34;&gt;Debiased Contrastive Learning of Unsupervised Sentence Representations&lt;/h1&gt;
&lt;p&gt;文章链接：
&lt;a href=&#34;https://arxiv.org/abs/2205.00656&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2205.00656&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：
&lt;a href=&#34;https://github.com/RUCAIBox/DCLR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/RUCAIBox/DCLR&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;摘要和引言&#34;&gt;摘要和引言&lt;/h2&gt;
&lt;p&gt;最近，对比学习被证明在改进预训练的语言模型（PLM）以获得高质量的句子表征方面是有效的。它的目的是拉近正样本以提高对齐度，同时推开不相关的负样本以提高整个表示空间的统一性。然而，以前的工作大多采用从一个batch内或从训练数据中随机抽取样本作为负样本。这样的方式可能会造成采样偏差，即不适当的负样本（如伪负样本和各向异性表征）被用来学习句子表征，这将损害表征空间的均匀性。为了解决这个问题，我们提出了一个新的框架DCLR（Debiased Contrastive Learning of unsupervised sentence Representations）来减轻这些不适当的负样本的影响。在DCLR中，我们设计了一种实例加权的方法来惩罚错误的负样本，并生成基于噪声的负样本来保证表示空间的统一性。在七个语义文本相似性任务上的实验表明，我们的方法比竞争性基线更有效。&lt;/p&gt;
&lt;p&gt;近来，预训练语言模型被广泛用于语义表征的实现方法，在多个NLP任务上取得了显著的表现。然而，一些研究发现，由PLM产生的原始句子表征在方向上并不是均匀分布的，而是在向量空间中形成一个狭窄的锥体（Ethayarajh，2019），这在很大程度上限制了其表现力。为了解决这个问题，对比学习（Chen等人，2020）已经被采用来完善PLM产生的句子表征。它将语义上接近的样本拉到一起，以提高一致性，同时将负样本推开，以提高整个表征空间的统一性。对于正样本，以前的工作在原句上应用了数据增强策略（Yan等人，2021），以产生高度相似的变化。而由于缺乏负样本的Golden Label，负面例子通常是从批处理或训练数据中随机抽取的（例如，批内负样本（Gao等人，2021））。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;QQ%E6%88%AA%E5%9B%BE20220518222943.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;虽然这种负样本的抽样方式简单方便，但它可能导致抽样偏差，影响句子表征的学习。首先，抽样的负样本很可能是虚假的负样本，而这些负样本在语义上确实与原句接近。如图1所示，根据SimCSE模型（Gao等人，2021年），给定一个输入句子，大约一半的批量内负样本与原句的余弦相似度超过0.7。通过简单地推开这些采样的负样本，很可能会伤害到句子表征的语义。其次，由于各向异性问题（Ethayarajh, 2019），采样负样本的表征来自PLMs所跨越的狭窄表征锥，不能完全反映表征空间的整体语义。因此，只依靠这些表征来学习句子表征的统一性目标是次优的。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，我们的目标是开发一种更好的对比学习方法，采用去偏的负采样策略。核心思想是改进随机抽样策略以缓解抽样偏差问题。首先，在我们的框架中，我们设计了一个实例加权的方法，以惩罚训练中的伪负样本。我们采用了一个补充模型来评估每个负样本与原句之间的相似性，然后为相似性较高的负样本分配较低的权重。通过这种方式，我们可以检测到语义上接近的假负样本，并进一步减少其影响。其次，我们根据随机的高斯噪声随机初始化新的负样本，以模拟整个语义空间内的抽样，并设计一个基于梯度的算法来优化基于噪声的负样本，以达到最不均匀的形式。通过与不均匀的基于噪声的负样本进行对比，我们可以扩展句子表征的空间，并提高表征空间的均匀性。&lt;/p&gt;
&lt;p&gt;为此，我们提出了DCLR，这是一个针对无监督句子表征的去偏向对立学习的一般框架。在我们的方法中，我们首先从高斯分布中初始化基于噪声的负样本，并利用基于梯度的算法来更新新的负样本，同时考虑代表空间的均匀性。然后，我们采用辅助模型来产生这些基于噪声的负样本和随机抽样的负样本的权重，其中错误的负样本将受到惩罚。最后，我们通过dropout（Gao等人，2021）来增加正面的例子，并将它们与上述加权的负面例子结合起来进行对比学习。我们证明了我们的DCLR在使用BERT（Devlin等人，2019）和RoBERTa（Liu等人，2019）的七个语义文本相似性（STS）任务上优于一些竞争基线。&lt;/p&gt;
&lt;p&gt;这项工作的目的是利用未标记的语料来学习有效的句子表征，可以直接用于下游任务，例如语义文本相似性任务（Agirre等人，2015）。给定一组输入句子X={x1, x2, . . . , xn}，我们的目标是以一种无监督的方式为每个句子xi学习一个代表hi∈Rd。为了简单起见，我们用一个参数化的函数hi = f(xi)来表示这个过程。&lt;/p&gt;
&lt;p&gt;在这项工作中，我们主要关注的是使用基于BERT的PLMs（Devlin等人，2019；Liu等人，2019）来生成句子表征。按照现有的工作（Li等人，2020年；Yan等人，2021年），我们通过我们提出的无监督学习方法在未标记的语料库上微调PLM。之后，对于每个句子xi，我们用微调后的PLM进行编码，并将最后一层的[CLS]标记的表示作为其句子表示hi。&lt;/p&gt;
&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;
&lt;p&gt;我们根据高斯分布初始化基于噪声的负样本，并迭代更新这些负样本，以达到非均匀性最大化。然后，我们利用一个辅助模型为所有的负样本（即随机抽样和基于噪声的负样本）产生权重。最后，我们将加权的负样本和增强的正面例子结合起来，进行对比学习。我们的DCLR的概述见图2。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;QQ%E6%88%AA%E5%9B%BE20220518230107.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;生成基于噪声的负样本&#34;&gt;生成基于噪声的负样本&lt;/h4&gt;
&lt;p&gt;我们的目标是在训练过程中，在PLMs的句子表征空间之外生成新的负样本，以减轻PLMs各向异性问题带来的采样偏差（Etha- yarajh，2019）。对于每个输入句子xi，我们首先从高斯分布中初始化k个噪声向量作为样本表征：
$$
{\hat h_1, \hat h_2,\cdots, \hat h_k}\sim \mathcal{N}(0, \sigma^2)
$$
其中σ是标准差。由于这些向量是从这样的高斯分布中随机初始化的，因此它们在整个语义空间中是均匀分布的。通过学习与这些新的负样本进行对比，有利于句子表征的均匀性。&lt;/p&gt;
&lt;p&gt;为了进一步提高新负样本的质量，我们考虑迭代更新负样本以捕捉整个语义空间中的非均匀性点。受VAT（Miy- ato等人，2017；Zhu等人，2020）的启发，我们设计了一个非均匀性损失最大化目标，以产生梯度来改善这些负样本。非均匀性损失被表示为基于噪声的负样本${\hat h_j}$和原句的正样本$ (h_i,h_i^+) $之间的对比损失：&lt;/p&gt;
&lt;p&gt;$$
L_U(h_i,h_i^+,{\hat h})=-\log\frac{e^{{\rm sim}(h_i,h_i^+)/\tau_u}}{\sum_{\hat h_j \in{\hat h}}e^{{\rm sim}(h_i, \hat h_i)/\tau_u}}
$$
其中$\tau_u$是温度超参数，${\rm sim}(h_i, h_i^+)$是余弦相似度$\frac{h_i^\top h_i^+}{||h_i||\cdot||h_i^+||}$。在此基础上，对于每个负样本$\hat h_j\in{\hat h}$，我们用$t$步梯度下降法优化它
$$
\hat h_j=\hat h_j+\beta g(\hat h_j)/||g(\hat h_j)||&lt;em&gt;2\
g(\hat h_j)=\nabla&lt;/em&gt;{\hat h_j}L_U(h_i,h_i^+,{\hat h})
$$&lt;/p&gt;
&lt;p&gt;这样一来，基于噪声的负样本将被优化到句子表征空间的非均匀点。通过学习与这些负样本的对比，可以进一步提高表示空间的均匀性，这对有效的句子表示是至关重要的。&lt;/p&gt;
&lt;h4 id=&#34;实例加权的对比学习&#34;&gt;实例加权的对比学习&lt;/h4&gt;
&lt;p&gt;除了上述基于噪声的负样本，我们还遵循现有的工作（Yan等人，2021；Gao等人，2021），采用其他batch内表征作为负样本${\tilde h^-}$。然而，正如前面所讨论的，采样的负样本可能包含与正样本有相似语义的例子（即伪负样本）。&lt;/p&gt;
&lt;p&gt;为了缓解这个问题，我们提出了一种实例加权的方法来惩罚伪负样本。由于我们无法获得真实的标签或语义相似性，我们利用一个补充模型来产生每个负数的权重。我们采用最先进的SimCSE（Gao等人，2021）作为补充模型。给定一个来自${\tilde h^-}$或${\hat h}$的负样本$h^-$和原始句子$h_i$的表征，我们利用辅助模型来产生权重：
$$
\alpha_{h^-}=\begin{cases}
0,{\rm sim}_C(h_i,h^-)\ge\phi\
1,{\rm sim}_C(h_i,h^-)&amp;lt;\phi
\end{cases}
$$
其中${\rm sim}&lt;em&gt;C$是辅助模型评估的余弦相似度。这样一来，与原句的语义相似度较高的负样本将被视为伪负样本，并被赋予0的权重以示惩罚。 基于这些权重，我们用去掉交叉熵的对比学习损失函数来优化句子表征，即
$$
L=-\log\frac{e^{{\rm sim}(h_i,h_i^+)/\tau}}{\sum&lt;/em&gt;{h^- \in{\hat h}\cup{\tilde h^-}}e^{{\rm sim}(h_i, h^-)/\tau}}
$$
如上所述，我们的方法旨在重新消除关于负面的抽样偏差的影响，并且对各种正样本的增强方法（例如，token cutoff和dropout）是不关心的。由于基于噪声的负样本是从高斯分布中初始化的，并不对应于真实的句子，因此它们是高度自信的负样本，以扩大表示空间。通过学习与它们的对比，对比目标的学习将不会受到来自PLMs的各向异性表征的限制。因此，句子表征可以跨越更广泛的语义空间，表征语义空间的统一性也可以得到改善。&lt;/p&gt;
&lt;p&gt;此外，我们的实例加权方法还缓解了随机抽样策略造成的伪负样本问题。在补充模型的帮助下，那些与原句语义相似的伪负样本句子将被检测出来并受到惩罚。&lt;/p&gt;
&lt;h2 id=&#34;实验&#34;&gt;实验&lt;/h2&gt;
&lt;p&gt;按照以前的工作（Kim等人，2021；Gao等人，2021），我们对七个标准的语义文本相似度任务（Semantic Textual Similarity，STS）进行了实验。对于所有这些任务，我们使用SentEval工具包（Conneau and Kiela, 2018）进行评估。&lt;/p&gt;
&lt;p&gt;我们在7个STS任务中评估了我们的方法。STS 2012-2016（Agirre等人，2012，2013，2014，2015，2016），STS Benchmark（Cer等人，2017）和SICK-Relatedness（Marelli等人，2014）。这些数据集包含成对的两个句子，其相似度分数从0到5进行标注。黄金标注和句子表征预测的分数之间的相关性由Spearman关联度来衡量。根据以往工作的建议（Gao等人，2021；Reimers和Gurevych，2019），我们直接计算所有STS任务的句子嵌入之间的余弦相似度。&lt;/p&gt;
&lt;h4 id=&#34;基线方法&#34;&gt;基线方法&lt;/h4&gt;
&lt;p&gt;我们将DCLR与竞争性的无监督句子表示学习方法进行比较，包括非BERT和基于BERT的方法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GloVe：把GloVe的词嵌入平均作为句子表征。&lt;/li&gt;
&lt;li&gt;USE：一个Transformer模型，训练目标是在一段话中重建周围的句子。&lt;/li&gt;
&lt;li&gt;CLS，Mean, First-Last AVG分别采用[CLS]嵌入、token表征的平均池化、第一层和最后一层的平均表征作为句子表征。&lt;/li&gt;
&lt;li&gt;Flow 将平均池化应用在各层表征上，并将输出映射到高斯空间上作为句子表征。&lt;/li&gt;
&lt;li&gt;Whitening使用whitening操作来完善表征，降低维度。&lt;/li&gt;
&lt;li&gt;Contrastive(BT)使用对比学习与回译进行数据增强，以提高句子的代表性。&lt;/li&gt;
&lt;li&gt;ConSERT探讨了各种文本增强策略，用于句子表征的对比学习。&lt;/li&gt;
&lt;li&gt;SG-OPT提出了一种具有自我引导机制的自适应学习方法，以改善PLM的句子嵌入。&lt;/li&gt;
&lt;li&gt;SimCSE提出了一个利用dropout进行数据增强的对比学习框架。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用从维基百科中随机抽取的1,000,000个句子作为训练语料库。&lt;/p&gt;
&lt;h4 id=&#34;实验分析&#34;&gt;实验分析&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;QQ%E6%88%AA%E5%9B%BE20220518232321.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;为了验证我们的框架在PLM上的有效性，我们选择BERT-base和RoBERTa-base作为基础模型。表1显示了不同方法在七个STS任务上的结果。根据这些结果，我们可以发现，非BERT方法(即GloVe和USE)大部分都比基于PLM表示的基线(即CLS，Mean和First-Last AVG)要好。原因是直接利用PLM的原始表征很容易受到各向异性的影响。在非BERT方法中，USE优于Glove。一个潜在的原因是USE使用Transformer模型对句子进行编码，这比简单的平均GloVe 嵌入更有效。&lt;/p&gt;
&lt;p&gt;对于其他基于PLM的方法，首先，我们可以看到flow和whiteening取得了类似的结果，并在一定程度上超过了基于原始表征的方法。这两种方法采用了特定的改进策略来完善PLMs的表征。第二，基于对比学习的方法在大多数情况下优于其他基线。对比学习可以提高语义相关的正样本对之间的一致性和使用负样本的表征空间的均匀性，从而产生更好的句子表征。此外，SimCSE在所有基线中表现最好。这表明dropout是一种比其他方法更有效的正样本增强方法，因为它很少伤害到句子的语义。&lt;/p&gt;
&lt;p&gt;最后，DCLR在大多数情况下都比所有的基线表现得更好，包括基于对比学习的方法。由于这些方法大多利用随机抽样的负样本（如batch内负样本）来学习所有句子表征的统一性，它可能会导致抽样偏差，如假负样本和各向异性的表征。与这些方法不同的是，我们的框架采用了一种实例加权的方法来惩罚错误的负样本，并采用了一种基于梯度的算法来生成基于噪声的负样本。这样一来，采样偏差问题可以得到缓解，我们的模型可以更好地学习均匀性，以提高句子表征的质量。&lt;/p&gt;
&lt;h4 id=&#34;扩展实验&#34;&gt;扩展实验&lt;/h4&gt;
&lt;p&gt;由于我们提出的DCLR是一个通用的框架，主要集中在无监督的句子代表的锥形学习的负采样上，它可以应用于其他依靠不同的正样本增强策略的方法。因此，在这一部分中，我们进行了实验，以检验我们的框架是否能够通过以下积极的数据增强策略带来改进：（1）Token Shuffling，随机打乱输入句子中的token顺序；（2）Feature/Token/Span Cutoff（Yan等人，2021），随机置零输入中的特征/token/token span；（3）Dropout，类似于SimCSE（Gao等人，2021）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;C:%5CUsers%5Cwzc%5CDesktop%5CQQ%E6%88%AA%E5%9B%BE20220518233537.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图3所示，我们的DCLR可以提高所有这些增强策略的性能，它显示了我们的框架与各种增强策略的有效性。此外，在所有的变体中，Dropout策略取得了最好的性能。这表明Dropout是一种更有效的增强高质量正样本的方法，也更适合于我们的方法。&lt;/p&gt;
&lt;h4 id=&#34;消融实验&#34;&gt;消融实验&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;QQ%E6%88%AA%E5%9B%BE20220518233719.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如表2所示，删除每个组件都会导致性能下降。这表明，在我们的框架中，实例加权法和基于噪声的负样本都很重要。但是，去除实例加权方法会导致更大的性能下降。其原因可能是错误的负样本对句子表示学习有较大的影响。此外，我们准备了三种变体进行进一步的比较：（1）Random Noise直接生成基于噪声的负样本，而没有基于梯度的优化；（2）Knowledge Distillation（Hinton等人，2015）利用SimCSE作为教师模型，在训练期间将知识提炼到学生模型中；（3）Self Instance Weighting采用模型本身作为补充模型来生成权重。从表2中，我们可以看到这些变化并没有像原始的DCLR那样表现良好。这些结果表明，第4节中提出的设计更适合我们的DCLR框架。&lt;/p&gt;
&lt;h4 id=&#34;均匀性分析&#34;&gt;均匀性分析&lt;/h4&gt;
&lt;p&gt;均匀性是句子表征的一个理想特性，描述了表征的均匀分布程度。为了验证我们框架的均匀性的改善，我们比较了DCLR和SimCSE在训练期间使用BERT-base的均匀性损失曲线。&lt;/p&gt;
&lt;p&gt;按照SimCSE（Gao等人，2021年），我们利用以下函数来评估均匀性：
$$
\ell_{uniform}\triangleq \mathop{\mathbb E}&lt;em&gt;{x_i,x_j \sim p&lt;/em&gt;{data}}\exp(-2||f(x_i)-f(x_j)||^2)
$$
这个损失的数值越小，说明均匀性越好。如图4所示，几乎在整个训练过程中，DCLR的均匀性损失要比SimCSE的低得多。此外，我们可以看到，随着训练的进行，DCLR的均匀性损失下降得更快，而SimCSE的均匀性损失则没有明显的下降趋势。这可能是因为我们的DCLR在表示空间之外对基于噪声的负样本进行了采样，这可以更好地提高句子表示的均匀性。&lt;/p&gt;
&lt;h4 id=&#34;少样本设置下的表现&#34;&gt;少样本设置下的表现&lt;/h4&gt;
&lt;p&gt;为了验证DCLR在数据稀缺情况下的可靠性和稳健性，我们使用BERT-base作为骨干模型进行了少样本实验。我们通过从100%到极小规模（即0.3%）的不同数量的可用训练数据训练我们的模型。我们报告了在STS-B和SICK-R任务上评估的结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;QQ%E6%88%AA%E5%9B%BE20220518234230.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图5所示，我们的方法在不同比例的训练数据下取得了稳定的结果。在数据比例为0.3%的最极端设置下，我们的模型在STS-B和SICK-R上的性能分别只下降了9%和4%。这些结果揭示了我们的方法在数据稀缺的情况下的稳健性和有效性。这种特性在现实世界的应用中是很重要的。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在本文中，我们提出了DCLR，一个用于无监督的去偏句子表征学习学习框架。我们的核心思想是缓解由随机负抽样策略引起的抽样偏差。为了实现这一目标，在我们的框架中，我们采用了一种实例加权的方法来惩罚训练过程中的错误负样本，并产生了基于噪声的负样本，以减轻各向异性的PLM衍生代表的影响。在七个STS任务上的实验结果表明，我们的方法优于几个有竞争力的基线。在未来，我们将探索其他方法来减少句子表征的对比性学习中的偏差。此外，我们还将考虑将我们的方法用于多语言或多模态表征学习。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DOP-Tuning: 面向对话摘要领域自适应的轻量级微调方法</title>
      <link>https://pris-nlp.github.io/en/post/220426zwh/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/220426zwh/</guid>
      <description>&lt;p&gt;本文介绍一下我们组在面向领域迁移的对话摘要任务上的工作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-motivations&#34;&gt;1. Motivations&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;当前的对话摘要模型往往缺乏在新领域上的泛化性，因为大规模的生成式预训练模型往往需要大量的人工标注的黄金摘要，在 few/no labeled 的场景下无法扩展到新的领域。&lt;/li&gt;
&lt;li&gt;当前研究摘要领域迁移的方法需要耗时的预训练和大规模额外的语料库。他们仅关注沉重的预训练步骤而不是轻量化的微调过程。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-contributions&#34;&gt;2. Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;我们第一个探索面向领域迁移的对话摘要任务的fine-tuning方法，并且在TODSum(TODSum是我们提出的对话摘要数据集
&lt;a href=&#34;https://arxiv.org/abs/2110.12680&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TODSum&lt;/a&gt;)和QMSum两个数据集上建立了实用且全面的benchmarks.&lt;/li&gt;
&lt;li&gt;提出了轻量且有效的面向领域的PrezZfix-tuning的模型，该模型使用领域词初始化的prefix模块以及离散的prompt来从大规模预训练模型中交互式地提取知识。&lt;/li&gt;
&lt;li&gt;进行了充分的实验和定量分析来证明了我们提出的方法的有效性，并且讨论了面向领域迁移的对话摘要所存在的挑战。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-methodology&#34;&gt;3. Methodology&lt;/h2&gt;
&lt;p&gt;模型结构包括Domain-Oriented Prefix，Prompt Encoder以及Decoder三个部分。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165057557-fda144c5-1bd3-4929-81c7-5671ab32ae79.png&#34; alt=&#34;89601b192ff24bbcb37aaf8f1b2338c6&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-domain-oriented-prefix&#34;&gt;3.1 Domain-Oriented Prefix&lt;/h3&gt;
&lt;p&gt;为了缓解领域耦合的问题，我们提出了domain-oriented的prefix模块来从源域和目标域中获取共享的知识。
采用two-step构建Domain-Oriented Prefix，包括初始化和参数化。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165057725-487201b1-46da-45f1-88a0-b529f04d6bb5.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;311-initialization&#34;&gt;3.1.1 Initialization&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;利用LDA主题模型从对话文本中提取每个领域的关键词，并且将他们拼接起来构成domain word（prefix）序列$x_{dw}$&lt;/li&gt;
&lt;li&gt;随机初始化domain word序列组成可学习的矩阵$M_{\theta}\in R^{|x_{dw}|d_{m}}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;312-parametrization&#34;&gt;3.1.2 Parametrization&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;利用MLP和预训练的BART模型分别得到domain word序列的表示，重新训练MLP使用MSE loss使得MLP的输出与预训练的BART的decoder hidden states相同，以此从预训练模型中解藕出领域知识。&lt;/li&gt;
&lt;li&gt;在更新MLP参数的过程中保持预训练的BART的参数固定。&lt;/li&gt;
&lt;li&gt;得到MLP的参数初始化，并使用预训练好的MLP来映射prefix表示的初始化embeddings.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;32-prompt-encoder&#34;&gt;3.2 Prompt Encoder&lt;/h3&gt;
&lt;h4 id=&#34;321-discrete-prompts&#34;&gt;3.2.1 Discrete Prompts&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;将TODSum数据集中的对话状态和QMSum中的queries作为离散的Prompts&lt;/li&gt;
&lt;li&gt;对于对话状态这种结构化的信息，将结构化的信息转化为文本序列。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;322-transformer-layer&#34;&gt;3.2.2 Transformer Layer&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;将离散的prompt序列$x_{dp}$以及对话文本序列$x_{d}$作为encoder的输入序列。&lt;/li&gt;
&lt;li&gt;通过修改添加domain-oriented prefix序列的键值对来修改self- attention机制。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;33-decoder&#34;&gt;3.3 Decoder&lt;/h3&gt;
&lt;p&gt;将Prefix模块也加到decoder上，以类似的方式修改cross-attention和self- attention机制。&lt;/p&gt;
&lt;h3 id=&#34;34-training-strategy&#34;&gt;3.4 Training Strategy&lt;/h3&gt;
&lt;p&gt;采用如下的训练目标更新梯度：&lt;/p&gt;
&lt;img width=&#34;805&#34; alt=&#34;e57e9ff9765e44f4b4e0a87ac6af44b3&#34; src=&#34;https://user-images.githubusercontent.com/47687248/165057976-c37eca89-bb9f-4c11-8b84-3fb21ba68e84.png&#34;&gt;
&lt;p&gt;在训练过程中中固定BART的参数，而更新prefix的参数。在训练时使用来自源域的领域词作为prefix序列。当训练完成以后，保存domain-oriented的prefix模块的参数，而丢弃掉预训练好的BART模块的参数。
在测试的过程中，目标域的领域词则被MLP模块映射为prefix表示。&lt;/p&gt;
&lt;h2 id=&#34;4-experimental-setup&#34;&gt;4. Experimental Setup&lt;/h2&gt;
&lt;h3 id=&#34;41--datasets&#34;&gt;4.1  Datasets&lt;/h3&gt;
&lt;p&gt;在两个multi-domain对话摘要数据集上评估了模型的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058127-321b66bc-711d-4f98-ac63-a45438f05462.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;411-todsum&#34;&gt;4.1.1 TODSum&lt;/h4&gt;
&lt;p&gt;TODSum是基于经典对话数据集MultiWOZ提出的task-oriented对话摘要数据集。根据领域信息，数据集可以被划为5个领域：restaurant，hotel，attraction，taxi以及train. 在实验时，选择5个域中的4个域作为源域，剩下的域作为目标域，从源域中抽取200个样本作为验证集，源域的剩余数据作为训练集，目标域的数据作为测试集。&lt;/p&gt;
&lt;h4 id=&#34;412-qmsum&#34;&gt;4.1.2 QMSum&lt;/h4&gt;
&lt;p&gt;QMSum数据集包括上千条会议录音数据，包括三个领域：academic，committee以及product. 采用类似于TODSum数据集的处理方式。&lt;/p&gt;
&lt;h3 id=&#34;42-main-results&#34;&gt;4.2 Main Results&lt;/h3&gt;
&lt;h4 id=&#34;421-results-on-todsum&#34;&gt;4.2.1 Results on TODSum&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058282-6f033a85-410c-410b-b48d-214a72a22313.png&#34; alt=&#34;4dbdb43c2e3142fd92523ca9aadec458&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看到，Prefix-Tuning相比较BART，BART w. DS.,表现要差，是因为对话文本很长且复杂，仅使用fine-tuning参数的20%很难理解领域知识，以及识别对话中的关键内容。在与Prefix-tuning具有相同量级的参数下，DOP- tuning在5个领域都有了较大的提升。这表明由领域词初始化的prefix模块以及有对话状态组成的离散的prompts发挥了重要的作用。除此之外，模型比全参数fine-tuning的模型BART要好，说明模型可以有效地从源域和目标域中解藕出知识。上述结果表示，在有限的数据情况下，模型仍然可以达到SOTA的结果。&lt;/p&gt;
&lt;h4 id=&#34;422-results-on-qmsum&#34;&gt;4.2.2 Results on QMSum&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058369-d1f8c780-8583-4396-b467-64d05c6b731d.png&#34; alt=&#34;b22e45cf03554374808330cbc773fe30&#34;&gt;&lt;/p&gt;
&lt;p&gt;整体表现的趋势与在TODSum数据集上的表现一致，但是可以看出在Rouges的分数相对而言较低，是因为领域并没有明显的领域词，导致了严重的领域耦合的问题。除此之外，由于会议文本过长，很难从对话中捕捉核心内容。总的来说，这些结果表明多领域设置对于会议摘要是非常必要且有意义的。&lt;/p&gt;
&lt;h2 id=&#34;5-qualitative-analysis&#34;&gt;5. Qualitative Analysis&lt;/h2&gt;
&lt;h3 id=&#34;51-number-of-domain-words&#34;&gt;5.1 Number of Domain Words&lt;/h3&gt;
&lt;p&gt;探究领域词数量的影响，可以看到领域词数量在140时使rouge达到了峰值，当低于140时，效果降低，说明参数量不足影响了模型的表现。当领域词数量超过阈值时，模型的表现下降，说明了太长的prefix序列给BART增加了负担，并且引入了额外的噪声。但是，领域词数量的变化并没有对模型的表现的有太大的影响（只有2～3%的起伏），说明了domain-oriented prefix模块和有效性和模型的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058462-5b0df7ca-b9cd-4e50-a36d-15d195fbc629.png&#34; alt=&#34;f31ac144b97549aa90d84813653b4ac8&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;52-quality-of-domain-words&#34;&gt;5.2 Quality of Domain Words&lt;/h3&gt;
&lt;p&gt;探究领域词的质量的影响，将领域词的中的一定比例的词语以与领域无关的词汇替代。可以看到，随着更多的噪声被引入，模型受到更大的影响且表现下降。当噪声的比例超过100%时，模型的表现甚至比Prefix- Tuning糟糕。这是因为，我们使用完全无关的词汇去初始化Prefix模块，相比较随机初始化引入了更多的噪声影响了DOP的表现。从这一点看，引入高质量的领域词有利于领域解藕，高质量的领域词对摘要生成是重要的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058527-b933ad77-3137-4b9d-ba45-ed3ec737cd1d.png&#34; alt=&#34;3303ad68395040e68b08203f4ac28d90&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;53-ablation-study&#34;&gt;5.3 Ablation Study&lt;/h3&gt;
&lt;p&gt;研究了domain-oriented initialization和discrete prompts的影响。同时去掉两个模块与原始prefix-tuning相同。可以看到去除prefix- tuning中的domain- oriented初始化会使模型表现严重下降，说明domain word信息在面对新领域时引入相关知识的重要性。同时，移除离散的prompts也会使模型表现更糟糕（但仍然会好于Prefix-Tuning），说明离散的prompts能让模型更关注对话中核心的内容进而提升模型的表现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058585-f8e3dd24-9f68-4cc5-b0cb-670ddb698351.png&#34; alt=&#34;faf6a973a06d4f64bd74f3c99becce12&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;54-effect-of-prefix-module-in-encoder-and-decoder&#34;&gt;5.4 Effect of Prefix Module in Encoder and Decoder&lt;/h3&gt;
&lt;p&gt;由于DOP-method在encoder和decoder中均引入了prefix模块，研究两个部分的prefix模块对模型表现的影响。可以看到，当两个部分的prefix被移除后，模型的表现均下降，说明了两个模块的prefix都是必要且高效的。&lt;/p&gt;
&lt;p&gt;一个有趣的现象是移除encoder的prefix的影响要小于移除decoder的prefix的影响。一个比较合理的解释是在encoder和decoder端的prefix的作用是不一样的。在encoder端的prefix主要帮助模型理解对话，而decoder端的prefix主要帮助模型生成。因此，对于摘要生成，decoder端的prefix模块对模型更有用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058661-66e0bf75-3766-41ed-9cf9-448b5e443e3a.png&#34; alt=&#34;4e9bda3719bb4c9bb210b920ab907f58&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;55-human-evaluation&#34;&gt;5.5 Human Evaluation&lt;/h3&gt;
&lt;p&gt;对模型进行了人工评估。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058716-dcb18574-ec70-40d7-ac51-725654fd3cd4.png&#34; alt=&#34;f757f1261bda4bceb881baa790173595&#34;&gt;&lt;/p&gt;
&lt;p&gt;表中显示，所有模型的流畅程度都较高，说明在较强的backbone上微调的抽象摘要模型能够生成更流畅的句子。在事实一致性上，DOP以及BART ws DS好于Prefix-tuning的表现，说明对话状态信息能引导模型更关注与核心的信息，例如槽值和意图。初次之外，DOP-tuning在领域相关性上的表现超过了其他基线模型。说明了domain-oriented模块在提升模型识别领域相关特征以及从源域和目标域解藕出知识的能力。&lt;/p&gt;
&lt;h3 id=&#34;56-effect-of-training-data&#34;&gt;5.6 Effect of Training Data&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058800-9f6323d4-ac48-406a-b256-017a18156ae4.png&#34; alt=&#34;20f9b8d72e8844abb49c580b1e47bdea&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;561-performance-in-few-shot-settings&#34;&gt;5.6.1 Performance in Few-shot Settings&lt;/h4&gt;
&lt;p&gt;对于TODSum数据集，固定源域的数据规模，将目标域的数据加入训练数据。可以看到随着目标域数据的增加，BART w. DS和DOP的表现提升，但DOP-tuning始终好于BART w. DS. 说明目标域的知识增加可以让模型学到目标域的信息。&lt;/p&gt;
&lt;h4 id=&#34;562-effect-of-source-domain-data-size&#34;&gt;5.6.2 Effect of Source Domain Data Size&lt;/h4&gt;
&lt;p&gt;保持zero-shot的设置不变，调整源域数据的规模&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058855-56a9767a-da09-4f77-aee4-6c83e43c58b0.png&#34; alt=&#34;fca012c5f1e441718b2181ace0c80153&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看到随着数据规模的减小，BART w. DS的表现变差，而DOP-tuning能够相对优秀地保持稳定。说明DOP-tuning对数据规模不太敏感，并且具有一定程度的鲁棒性。这与主实验的结果一致，模型在有限和unseen data上表现优异。&lt;/p&gt;
&lt;h3 id=&#34;57-prefix-length-vs-input-length&#34;&gt;5.7 Prefix Length vs. Input Length&lt;/h3&gt;
&lt;p&gt;研究Prefix Length和input Length的关系，具体而言source input length，target input length以及对应的optimal prefix length的关系。可以得出更长的inputs可能更青睐于更短的prefix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/47687248/165058896-b39fa528-04f3-4dd6-85da-a32c49d54a81.png&#34; alt=&#34;1d1a29e6f80b4d3b90e22b10c45c5215&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;6-challenges&#34;&gt;6. Challenges&lt;/h2&gt;
&lt;p&gt;总结了抽象对话摘要的低资源领域迁移的挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Confusion between domains with high similarity
对于词汇表高度重合的领域，如restaurant和hotel，train和taxi，模型会产生domain-confusing句子。以hotel- restaurant对作为例子，当restaurant作为目标域，“book a restaurant room that can accommodate 3 people”会被生成，这样的句子其实更应该存在hotel领域中。但需要注意的是，这种challenge并不会影响关键因素的准确率，但language style则是不合适的。&lt;/li&gt;
&lt;li&gt;Information dispersion
由于对话数据通常是长序列，因此模型很难对长对话中的所有方面都能pay attention，因此会产生对话中关键元素的注意力的偏差，尤其是在轻量和小参数训练的场景下。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;7-conclusion&#34;&gt;7. Conclusion&lt;/h2&gt;
&lt;p&gt;在本文中，我们提出了基于高效且可泛化的微调方法面向领域的domain-oriented prefix-tuning模型解决对话摘要中的领域迁移的方法。使用领域词初始化的prefix模块能够从源域解藕出目标域的知识，而离散的prompts能够提升模型的泛化性。在zero-shot和few-shot下的实验说明我们的方法在两个数据集下取得了巨大的进步。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recent research overview（2022.04.08）</title>
      <link>https://pris-nlp.github.io/en/post/220408wzc/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/220408wzc/</guid>
      <description>&lt;h2 id=&#34;adpl-adversarial-prompt-based-domain-adaptation-for-dialogue-summarization-with-knowledge-disentanglement&#34;&gt;&lt;strong&gt;ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weihao-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾伟豪&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/ruotong-geng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;耿若彤&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://sigir.org/sigir2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SIGIR 2022&lt;/a&gt; Full Paper&lt;/p&gt;
&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;领域自适应是机器学习中的一个基本任务。在本文中，我们研究对话摘要任务中的领域迁移问题，试图借助源域的有标注数据迁移到无标注或少标注的目标域，进而提升低资源目标域下对话摘要的生成效果，可用于解决实际场景中小业务数据匮乏的挑战。传统的对话摘要领域迁移方法往往依赖于大规模领域语料，借助于预训练来学习领域间知识。该方法的缺点是实际语料收集难，对算力要求高，针对每一个目标域都需要进行耗时的预训练过程，效率低。因此，本文从微调的角度出发，提出了一种轻量级的解耦知识迁移方法ADPL，无需大规模的预训练过程，仅仅利用源域数据和少量的无标注目标域数据，即可实现高质量的对话摘要生成。具体来说，我们基于prompt learning的思想，针对对话摘要任务中的领域迁移问题，提出了三种特定的prompt结构：domain-invariant prompt (DIP), domain-specific prompt (DSP), 和task-oriented prompt (TOP)，其中 DIP 用来捕获领域间的共享特征，DSP用来建模领域特有知识，TOP用来促进生成流畅的摘要。在训练中，我们仅仅更新这些prompt相关的参数就可以实现领域间知识的解耦和迁移，相比较之前的预训练方法，训练高效环保，对机器的显存要求显著降低。同时，我们基于两个大规模的对话摘要数据集QMSum和TODSum构建了对话摘要领域迁移评测集，在两个评测集上取得了一致的最优效果，实验结果和消融分析都证明了本文提出方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;center&gt;&lt;font face=&#34;黑体&#34; size=3&gt;ADPL整体框架示意图&lt;/font&gt;&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;revisit-overconfidence-for-ood-detection-reassigned-contrastive-learning-with-adaptive-class-dependent-threshold&#34;&gt;&lt;strong&gt;Revisit Overconfidence for OOD Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-1&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref2:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref3:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yuanmeng-yan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;严渊蒙&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/qixiang-gao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;高琪翔&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/zhiyuan-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref1:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-1&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://2022.naacl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAACL 2022&lt;/a&gt; Main Conference, Long Paper&lt;/p&gt;
&lt;h4 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;在面向任务的对话系统中，域外意图（out-of-domain, OOD）检测是必不可少的。它旨在检测用户查询是否超出预定义意图范围（in-domain, IND），以避免执行错误的操作。由于OOD意图标注的复杂性，大多数工作都集中在无监督OOD检测上，即没有标记OOD数据，只有标记IND数据。而当前无监督域外检测方法均忽略了域外检测中的关键性挑战——神经网络的过自信问题。&lt;/p&gt;
&lt;p&gt;在本文中，我们对过自信问题进行了深入分析，并将其拆解为两方面：过自信OOD和过自信IND。基于此，我们分别提出了一种新的重分配对比学习(RCL)来区分语义相似的IND类别之间的意图表示，以缓解过自信OOD问题，以及一种自适应的类局部阈值机制来区分相似的IND和OOD样本，以缓解过自信IND问题。&lt;/p&gt;
&lt;p&gt;具体来说，对于过自信OOD问题，我们首先使用预训练的意图分类器在易混淆的IND类型中构建hard对比对（其中，具有相同真实标签但不同的预测标签称为hard positive pair，不同真实标签但相同预测标签称为hard negative pair），然后，基于构建的对比对训练一个新的模型，并通过监督对比学习来学习相似IND类别的鉴别性意图表示；对于过自信IND问题，不同于传统MSP、GDA使用单一全局阈值的方式，为考虑IND和OOD类别之间的关联性，我们为每个意图类别自适应赋予一个独立阈值，有效区分语义高度相似的IND和OOD样本，缓解过自信OOD问题。多个数据集上实验和分析证明了本文提出方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;domain-oriented-prefix-tuning-towards-efficient-and-generalizable-fine-tuning-for-zero-shot-dialogue-summarization&#34;&gt;&lt;strong&gt;Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-2&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/lulu-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;赵璐璐&lt;/a&gt;&lt;sup id=&#34;fnref4:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/fujia-zheng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;郑馥嘉&lt;/a&gt;&lt;sup id=&#34;fnref5:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weihao-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾伟豪&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref2:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-2&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://2022.naacl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAACL 2022&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;introduction-2&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;现实生活中经常面临到新领域中数据标注稀缺的问题，为新领域进行标注耗时耗力，因此利用有限的源域注释数据为目标域开发低资源对话摘要模型至关重要。当前的生成式对话摘要方法缺乏对新领域的泛化能力，而现有的在摘要领域自适应问题上的研究通常是依赖于大规模的二次预训练。&lt;/p&gt;
&lt;p&gt;为了探索对话摘要领域自适应的轻量级微调方法，在本文中，我们提出了一种高效且可泛化的面向领域的Prefix-tuning模型（Domain-Oriented Prefix-tuning，DOP），在冻结的预训练模型的基础上，结合连续的prefix和离散的prompt表示，提高模型的领域自适应能力。具体来说，我们使用无监督 LDA提取的领域词来初始化连续提示向量，以获得前缀模块的初始参数和表示。我们还添加了一个面向领域的键值对前缀序列来增强经典注意力层，以交互方式获取知识并实现优化。除此之外，我们使用dialogue state和query作为离散prompt，引导模型关注对话中关键内容并增强对新领域的泛化能力。&lt;/p&gt;
&lt;p&gt;我们在两个多域对话摘要数据集 TODSum 和 QMSum 上进行零样本迁移实验并建立领域自适应benchmark， 充分的实验和定性分析证明了我们方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;disentangled-knowledge-transfer-for-ood-intent-discovery-with-unifified-contrastive-learning&#34;&gt;&lt;strong&gt;Disentangled Knowledge Transfer for OOD Intent Discovery with Unifified Contrastive Learning&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;authors-3&#34;&gt;Authors&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yutao-mu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牟宇滔&lt;/a&gt;&lt;sup id=&#34;fnref6:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/keqing-he/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;何可清&lt;/a&gt;&lt;sup id=&#34;fnref7:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/yanan-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;吴亚楠&lt;/a&gt;&lt;sup id=&#34;fnref8:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/zhiyuan-zeng/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;曾致远&lt;/a&gt;，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/hong-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐红&lt;/a&gt;，江会星，武威，
&lt;a href=&#34;https://pris-nlp.github.io/en/author/weiran-xu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;徐蔚然&lt;/a&gt;&lt;sup id=&#34;fnref3:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4 id=&#34;conference-3&#34;&gt;Conference&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.2022.aclweb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACL 2022&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;introduction-3&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;本文研究域外(OOD)意图发现任务，该任务旨在将新出现的未知意图的样本按意图语义聚成不同类簇，这有助于任务型对话系统发展新的技能。不同于传统的文本聚类任务，域外意图发现需要考虑如何利用已知的域内(IND)意图类别的先验知识，帮助新意图的聚类。因此相关方法都遵循一个两阶段框架：IND预训练和OOD聚类。其中的关键挑战在于如何将IND先验知识迁移到OOD聚类上。之前的方法普遍存在一个问题，将IND预训练过程当作分类任务，采用一个交叉熵分类损失，模型学习到如何分类IND样本，但是下游我们需要对OOD聚类。两个阶段不同的学习目标使得存在一个天然的语义鸿沟，使得IND到OOD的知识迁移变得困难。此外，我们观察到之前的方法只迁移BERT输出的一个共享意图表征，考虑到表征具有高度耦合性，这样一个表征可能不利于OOD聚类。例如，在IND预训练的编码器中存在实例级(instance-level)和聚类级(class-level)的知识，解耦不同级别的知识有助于更好地知识迁移。为了解决这样的问题，我们建立了一个统一的多头对比学习框架，并在此基础上提出了一个新颖的解耦知识迁移方法(DKT)，以便迁移解耦IND意图知识用于OOD聚类。我们意在弥合IND预训练和OOD聚类两个阶段的语义鸿沟。两个基准数据集的实验和分析显示了我们方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Equal Contribution&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref4:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref5:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref6:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref7:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref8:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Corresponding Author&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref3:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Measure and Improve Robustness in NLP Models: A Survey</title>
      <link>https://pris-nlp.github.io/en/post/211227wzc/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://pris-nlp.github.io/en/post/211227wzc/</guid>
      <description>&lt;!-- 如果你想外链到其他博客分享论文，插入超链接即可
[一个帅哥的论文分享](https://helicqin.github.io/2021/08/08/Modeling%20Fine-Grained%20Entity%20Types%20with%20Box%20Embeddings/) --&gt;
&lt;!-- 当然你也可以直接在这里写博客，像下面这样 --&gt;
&lt;p&gt;本文是一片关于NLP模型鲁棒性的综述，统一地介绍了如何定义、衡量和提升NLP模型的鲁棒性。&lt;/p&gt;
&lt;h2 id=&#34;nlp模型鲁棒性的定义&#34;&gt;NLP模型鲁棒性的定义&lt;/h2&gt;
&lt;p&gt;模型在$(x, y) \sim \mathcal{D}$上训练，在$(x&amp;rsquo;, y&amp;rsquo;)\sim \mathcal{D&amp;rsquo;} \ne \mathcal{D}$
上测试，可以用模型在$\mathcal{D&amp;rsquo;}$上的性能（如准确率）衡量模型的鲁棒性。&lt;/p&gt;
&lt;p&gt;可以粗略地将现有的模型鲁棒性的文献分为两类：$\mathcal{D&amp;rsquo;}$是对输入合成扰动，或者
$\mathcal{D&amp;rsquo;}$是自然发生的分布转移。&lt;/p&gt;
&lt;h3 id=&#34;对对抗攻击的鲁棒性&#34;&gt;对对抗攻击的鲁棒性&lt;/h3&gt;
&lt;p&gt;对抗攻击是指故意精心制造噪声来欺骗模型做出错误的预测，之前在CV领域被广泛探索，
后来扩展到NLP领域。对抗样本的生成主要建立在这样一个观察之上，即我们可以生成对人类有意义的样本(例如，通过用人类察觉不到的变化干扰样本)，同时改变对该样本的模型预测。
对抗攻击主要建立在人类可以理解大量同义词或者忽略字母的确切顺序，但机器不能。&lt;/p&gt;
&lt;p&gt;现在大多数CV研究都做了一个相对简单的假设，即在$x$上加有界摄动得到的$x&amp;rsquo;$的金标应该保持不变，
即$y&amp;rsquo;=y$，模型的鲁棒行为应该是$f(x&amp;rsquo;)=f(x)$。其中的摄动可以是token和字符的交换，释义，
不改变语义的对抗规则，或者添加干扰因素。&lt;/p&gt;
&lt;p&gt;然而，这个标签不变的假设可能并不总是成立的，有人研究了几种现有的文本扰动技术，但发现
一大部分扰动样本都改变了标签（尽管是在保留标签的假设下）或者结果的标签在人类标注者中
存在高度分歧。&lt;/p&gt;
&lt;p&gt;还有一个相似的概念是语义保留，是指$(x, x&amp;rsquo;)$之间的语义是不变的，而上面的标签保留是指
$(y, y&amp;rsquo;)$是不变的。&lt;/p&gt;
&lt;h3 id=&#34;对分布转移的鲁棒性&#34;&gt;对分布转移的鲁棒性&lt;/h3&gt;
&lt;p&gt;现有的鲁棒性定义更接近域泛化或OOD泛化的概念，其中测试集（不管有无标签）都是在训练的时候
不可获取的。
在NLP背景下，对自然分布转移的鲁棒性也意味着模型的性能不会因为语法错误、方言、说话者和
语言的差异或者为同一任务不同领域新收集的数据集而降低。&lt;/p&gt;
&lt;p&gt;另一个密切相关的研究方向是公平性，例如，在指代消解、职业分类、神经机器翻译等任务中观察到了性别刻板印象或偏差。&lt;/p&gt;
&lt;h3 id=&#34;联系和共同主题&#34;&gt;联系和共同主题&lt;/h3&gt;
&lt;p&gt;合成
上述两个分类都可以归为一个框架中，即$\mathcal{D&amp;rsquo;}$代表合成分布转移（通过对抗攻击）或自然分布转移。
两者的联系仍有待探索。在CV领域，有研究表面对合成分布转移的鲁棒性可能对自然分布转移鲁棒性贡献
很少甚至没有贡献。&lt;/p&gt;
&lt;p&gt;为了更好地理解模型为什么缺乏鲁棒性，一些现有的工作认为这是因为模型有时利用虚假的特征和标签之间的
关系，而非真正的关系。其中虚假的特征通常定义为并不真正影响任务标签的特征。它们和任务标签有联系，但是不能转移到更加有挑战性的测试条件或OOD数据。一些其他的工作将其定义为适用于大多数情况但不适用于一般情况的规则。这些虚假的关系有时被称为数据集偏差或群体转移。此外，有证据表明，控制模型在虚假特征下的学习将提高模型在分布转移中的性能。还有学者讨论了对抗鲁棒性和伪特征学习之间的联系。还有人通过将模型在分布转移或对抗性攻击中缺乏鲁棒性的原因归因于模型对虚假特征的学习，提出了连接这些领域的理论性的讨论。&lt;/p&gt;
&lt;p&gt;此外，在某些应用中，模型的鲁棒性也可以与模型的不稳定性或者模型的不确定度估计很差联系起来。
对于此，有人提出了贝叶斯方法、基于dropout和基于组装的实现方法。最近，Ovadia等人已经证明，模型的不确定性估计可能在分配转移下显著降低，并呼吁应当通过对OOD数据给出更低的不确定性估计，确保
模型“知道它什么时候不知道”。&lt;/p&gt;
&lt;h2 id=&#34;识别非鲁棒&#34;&gt;识别非鲁棒&lt;/h2&gt;
&lt;p&gt;随着鲁棒性在自然语言处理文献中得到越来越多的关注，各行各业都提出了识别自然语言处理模型鲁棒性失效的方法。现有工作可以根据故障的识别方式大致分类，其中很大一部分工作依赖于人类先验和对现有NLP模型的错误分析，其他一些工作线采用基于模型的方法。为了更准确地度量自然语言处理模型的鲁棒性，通常将识别出的鲁棒性失效模式组织成具有挑战性/对抗性的基准数据集。表1展示了用于识别模型鲁棒性失效的常用扰动类型（数据集），在表2中，我们总结了每个NLP任务的常见鲁棒性基准（benchmark）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2021/12/27/vwJmq2xBXTMHcfO.png&#34; alt=&#34;fig&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;人类先验和错误分析驱动的&#34;&gt;人类先验和错误分析驱动的&lt;/h3&gt;
&lt;p&gt;按任务分，包括NLI（Natural Language Inference），QA和神经机器翻译。&lt;/p&gt;
&lt;h4 id=&#34;nli&#34;&gt;NLI&lt;/h4&gt;
&lt;p&gt;Naik等人抽样错误分类的例子，并分析其潜在的错误来源，然后将其归类为常见错误原因的类型。这些错误类型将作为构建压力测试集的基础，以进一步评估NLI模型是否具有做出真实推理决策的能力，还是仅仅依赖于复杂的模式匹配。Gururangan等人发现，目前的NLI模型很可能仅依靠假设来识别标签，而Poliak等人提供了类似的补充，即我们采用仅假设的模型可以优于一组强基线。&lt;/p&gt;
&lt;h4 id=&#34;qa&#34;&gt;QA&lt;/h4&gt;
&lt;p&gt;有人提出通过在段落末尾串联一个敌对的干扰句来生成敌对的QA例子。Miller等人为斯坦福问答数据集(SQuAD)构建了四个新的测试集，并发现大多数问答系统未能推广到这一新的数据。他们也希望对自然分布转移进行新的评估指标。&lt;/p&gt;
&lt;h4 id=&#34;机器翻译&#34;&gt;机器翻译&lt;/h4&gt;
&lt;p&gt;Belinkov和Bisk发现，当面对嘈杂数据时，基于字符的神经机器翻译(NMT)模型是脆弱的，很容易不稳定，其中噪音(例如，打字、拼写错误等)是使用可能的词汇替换合成的。使用包含人工引入语法错误的句子或随机合成噪音的增强训练数据可以使系统对这种虚假模式更加稳健。另一方面，有人展示了另一种方法，通过限制字符的输入空间，使模型有可能感知数据输入错误和拼写错误。&lt;/p&gt;
&lt;h3 id=&#34;基于模型的&#34;&gt;基于模型的&lt;/h3&gt;
&lt;p&gt;这种方法有的是任务不确定的，有的是输入不可知的。
这种方法通过训练一个额外的模型来捕获偏差。例如，在视觉问题回答中，Clark等人训练一个朴素模型来预测基于问题的原型答案，而不考虑上下文;Utama等人提出学习一个仅使用数据集偏差相关特征的有偏模型。此外，Culotta的目标是通过训练分类器来识别模型中的捷径，从而从人类标注的例子中更好地区分虚假的相关性和真实的相关性。&lt;/p&gt;
&lt;h2 id=&#34;提升模型鲁棒性&#34;&gt;提升模型鲁棒性&lt;/h2&gt;
&lt;p&gt;根据人工干预的位置和方式，这些方法可以分为数据驱动的、基于模型和训练方案的、基于归纳先验和最后的因果干预。&lt;/p&gt;
&lt;h3 id=&#34;数据驱动数据增强&#34;&gt;数据驱动（数据增强）&lt;/h3&gt;
&lt;p&gt;如Mixup, MixText, CutOut, AugMix, HiddenCut。这类缓解方法是在数据层面上操作的，往往很难解释如何以及为什么起作用。&lt;/p&gt;
&lt;h3 id=&#34;基于模型和训练策略&#34;&gt;基于模型和训练策略&lt;/h3&gt;
&lt;h4 id=&#34;预训练&#34;&gt;预训练&lt;/h4&gt;
&lt;p&gt;最近的研究表明，预训练是提高NLP模型非分布鲁棒性的有效方法，这可能是因为其自我监督的目标，以及使用了大量不同的训练前数据，这些数据鼓励从少量的测试样本中归纳出一般化的结果，从而抵消了虚假的相关性。有研究显示一些其他因素也可以促进稳健的准确性，包括更大的模型尺寸、更多的微调数据和更长的微调。Taori等人在视觉领域也进行了类似的观察，其中作者发现，与现有文献提出的各种鲁棒性干预相比，使用更大、更多样化的数据集的训练在多个情况下提供了更好的鲁棒性。&lt;/p&gt;
&lt;h4 id=&#34;更好地利用少数群体的样本训练&#34;&gt;更好地利用少数群体的样本训练&lt;/h4&gt;
&lt;p&gt;还有一些工作建议通过更好地使用少数例子来简化模型，例如，在训练分布中代表性不足的例子，或更难学习的例子。例如，Yaghoobzadeh等人提出，首先根据全部数据对模型进行微调，然后只对少数例子进行微调。一般来说，强调样本子集的训练策略对模型来说特别难学习，有时也被称为DRO（distributional robust optimization，分布式鲁棒优化）组。DRO的扩展主要是讨论如何识别被认为是少数样本。例如，Nam等人通过强调模型的早期决策来训练另一个模型;Lahoti等人也使用另一个模型来识别对主模型具有挑战性的样本;Liu等人提出通过对在第一次训练时损失较大的少数例子增加权重，对模型进行第二次训练。&lt;/p&gt;
&lt;h3 id=&#34;基于归纳的实现方法&#34;&gt;基于归纳的实现方法&lt;/h3&gt;
&lt;p&gt;另一个思路是引入归纳偏差(即正则化假设空间)，迫使模型丢弃一些虚假的特征。这与基于人先验的识别方法密切相关，因为这些人的先验知识通常可以用额外的正则化器来重新制定训练目标。为了实现这一目标，通常需要首先构造一个侧组件来通知主模型有偏差的特征，然后根据侧组件来正则化主模型。这个侧组件的构造通常依赖于失调特征是什么的先验知识。然后，可以建立相应的方法来应对特征。类似地，Clark等人提出用一个显式捕获偏差的模型进行集成，其中主模型与这个仅偏差（bias-only）模型一起训练，这样主模型就不被鼓励使用偏差。最近的工作表明，通过更好地校准仅偏差模型，基于集成的方法可以进一步改进。&lt;/p&gt;
&lt;p&gt;总之，这种方法大概是在不同的领域/分布中训练小的经验损失，以迫使模型对特定领域的虚假特征不变。&lt;/p&gt;
&lt;h3 id=&#34;干预因果关系&#34;&gt;干预因果关系&lt;/h3&gt;
&lt;p&gt;Srivastava利用人类对因果关系的常识知识，以潜在的未测量变量来增加训练样本，并提出了一种基于DRO的方法，以使模型对分布转移具有鲁棒性。Veitch等人提出学习依赖于数据因果结构的估计反事实不变量预测，并表明它可以帮助减少文本分类中的虚假相关性。&lt;/p&gt;
&lt;h3 id=&#34;提升鲁棒性策略的联系&#34;&gt;提升鲁棒性策略的联系&lt;/h3&gt;
&lt;p&gt;大体可分为3类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用大量数据预训练模型&lt;/li&gt;
&lt;li&gt;学习跨领域/环境的不变表示或预测&lt;/li&gt;
&lt;li&gt;基于具体的虚假/偏见模式的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有趣的是，统计研究表明，许多缓解方法都有相同的鲁棒机器学习泛化误差界。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
