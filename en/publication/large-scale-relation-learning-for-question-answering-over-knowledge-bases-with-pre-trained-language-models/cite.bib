@inproceedings{yan-etal-2021-large,
    title = "Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models",
    author = "Yan, Yuanmeng  and
      Li, Rumei  and
      Wang, Sirui  and
      Zhang, Hongzhi  and
      Daoguang, Zan  and
      Zhang, Fuzheng  and
      Wu, Wei  and
      Xu, Weiran",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.296",
    pages = "3653--3660",
    abstract = "The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.",
}