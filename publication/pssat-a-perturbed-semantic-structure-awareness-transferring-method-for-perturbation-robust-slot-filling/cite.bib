@inproceedings{dong-etal-2022-pssat,
    title = "{PSSAT}: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling",
    author = "Dong, Guanting  and
      Guo, Daichi  and
      Wang, Liwen  and
      Li, Xuefeng  and
      Wang, Zechen  and
      Zeng, Chen  and
      He, Keqing  and
      Zhao, Jinzheng  and
      Lei, Hao  and
      Cui, Xinyue  and
      Huang, Yi  and
      Feng, Junlan  and
      Xu, Weiran",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.473",
    pages = "5327--5334",
    abstract = "Most existing slot filling models tend to memorize inherent patterns of entities and corresponding contexts from training data. However, these models can lead to system failure or undesirable outputs when being exposed to spoken language perturbation or variation in practice. We propose a perturbed semantic structure awareness transferring method for training perturbation-robust slot filling models. Specifically, we introduce two MLM-based training strategies to respectively learn contextual semantic structure and word distribution from unsupervised language perturbation corpus. Then, we transfer semantic knowledge learned from upstream training procedure into the original samples and filter generated data by consistency processing. These procedures aims to enhance the robustness of slot filling models. Experimental results show that our method consistently outperforms the previous basic methods and gains strong generalization while preventing the model from memorizing inherent patterns of entities and contexts.",
}